@inproceedings{Aly2012,
address = {New York, New York, USA},
author = {Aly, Amir and Tapus, Adriana},
booktitle = {Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction - HRI '12},
doi = {10.1145/2157689.2157783},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Aly, Tapus - 2012 - Prosody-driven robot ARM gestures generation in human-robot interaction.pdf:pdf},
isbn = {9781450310635},
keywords = {chmm,human-robot interaction,nao,non-verbal and para-verbal mapping},
mendeley-tags = {nao},
month = mar,
pages = {257},
publisher = {ACM Press},
title = {{Prosody-driven robot ARM gestures generation in human-robot interaction}},
url = {http://dl.acm.org/citation.cfm?id=2157689.2157783},
year = {2012}
}
@inproceedings{Bremner2009,
abstract = {In order for anthropomorphic robots to communicate with people in a human-like way they need to produce gestures along with speech. Beat gestures are a key category of gestures, accompanying emphasis and timing of talk. The standard approach for autonomously adding gestures to speech for artificial agents to perform, is to identify when gestures should occur, and then select appropriate movements from a prewritten library. However, selecting naively from a library is unlikely to result in particularly human-like gesture sequences. In order to obtain examples of engaging human beat gestures, we studied videos of chat show hosts. Our analysis reveals rules for the creation of human-like beat gestures. We have combined these rules with hand scripted non-beat gestures, to produce a monologue with a complete set of accompanying gestures; it is designed for performance by our humanoid robot BERTI. In order to test the gestures produced we carried out a pilot study at the London Science Museum. The results of the study indicate that having correctly designed gesture sequences improves observer engagement.},
author = {Bremner, P. and Pipe, A. G. and Fraser, M. and Subramanian, S. and Melhuish, C.},
booktitle = {RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2009.5326136},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Bremner et al. - 2009 - Beat gesture generation rules for human-robot interaction.pdf:pdf},
isbn = {978-1-4244-5081-7},
issn = {1944-9445},
keywords = {BERTI humanoid robot,BERTI robot,Cognitive robotics,Human robot interaction,Humanoid robots,Libraries,London Science Museum,Motion analysis,Production systems,Psychology,Speech analysis,Timing,Torso,anthropomorphic robots,artificial agents,beat gesture generation rules,human-like beat gestures,human-like gesture sequences,human-robot interaction,humanoid robots,prewritten library},
mendeley-tags = {BERTI robot},
month = sep,
pages = {1029--1034},
publisher = {IEEE},
shorttitle = {Robot and Human Interactive Communication, 2009. R},
title = {{Beat gesture generation rules for human-robot interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5326136},
year = {2009}
}
@inproceedings{Bremner2011,
abstract = {Co-verbal gestures, the spontaneous gestures that accompany human speech, form an integral part of human communications; they have been shown to have a variety of beneficial effects on listener behaviour. Therefore, we suggest that a humanoid robot, which aims to communicate effectively with human users, should gesture in a human-like way, and thus engender similar beneficial effects on users. In order to investigate whether robot-performed co-verbal gestures do produce these effects, and are thus worthwhile for a communicative robot, we have conducted two user studies. In the first study we investigated whether users paid attention to our humanoid robot for longer when it performed co-verbal gestures, than when it performed small arm movements unrelated to the speech. Our findings confirmed our expectations, as there was a very significant difference in the length of time that users paid attention between the two conditions. In the second user study we investigated whether gestures performed during speech improved user memory of facts accompanied by gestures and whether they were linked in memory to the speech they accompanied. An observable affect on the speed and certainty of recall was found. We consider these observations of normative responses to the gestures performed, to be an indication of the value of co-verbal gesture for a communicative humanoid robot, and an objective measure of the success of our gesturing method.},
annote = {To justify the importance},
author = {Bremner, Paul and Pipe, Anthony G. and Melhuish, Chris and Fraser, Mike and Subramanian, Sriram},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100810},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Bremner et al. - 2011 - The effects of robot-performed co-verbal gesture on listener behaviour.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {2164-0572},
keywords = {Humanoid robots,Humans,Robot kinematics,Speech,Torso,Videos,communicative robot,gesture recognition,human communications,human speech,human-robot interaction,humanoid robot,humanoid robots,justification,listener behaviour,robot-performed coverbal gesture effect,speech processing},
mendeley-tags = {justification},
month = oct,
pages = {458--465},
publisher = {IEEE},
shorttitle = {Humanoid Robots (Humanoids), 2011 11th IEEE-RAS In},
title = {{The effects of robot-performed co-verbal gesture on listener behaviour}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100810},
year = {2011}
}
@inproceedings{Bremner2009a,
abstract = {The human sciences have demonstrated that gesture is a critical element of human communication. While existing graphical solutions are appropriate for virtual agents, solving arm trajectories for physically embodied robots requires that we consider the challenges of robot dynamics within a realtime gesture framework. We explore and evaluate a low computational-cost gesture production algorithm that can generate adequate gesture trajectories in a humanoid torso, as judged by participants in Human-Robot gesturing studies presented in this paper. Our approach produces a constrained inverse-kinematic solution for the start and end points, and generates appropriate wrist angles. Gesture time is used to calculate the joint accelerations to give a smooth, direct hand movement. Selecting open hand gestures as an example gesture sub-domain, we implement our controller on BERTI, a bespoke upper-torso humanoid robot (Fig. 2). A qualitative pilot study highlights gesture features salient to users: gesture shape, timing, naturalness and smoothness. A controlled experimental study then demonstrates that, by these metrics, our algorithm performs well; despite some dissimilarities with users' own gestures. We establish some salient points of robot gestures based on these studies.},
author = {Bremner, Paul and Pipe, Anthony and Melhuish, Chris and Fraser, Mike and Subramanian, Sriram},
booktitle = {2009 IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.2009.5346903},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Bremner et al. - 2009 - Conversational gestures in human-robot interaction.pdf:pdf},
isbn = {978-1-4244-2793-2},
issn = {1062-922X},
keywords = {Anthropomorphism,BERTI,BERTI robot,Conversational Gesture,Human-Robot Interaction,Humanoid robots,Humans,Job production systems,Kinematics,Libraries,Low-Cost Motion,Motion analysis,Speech,Torso,Wrist,arm trajectories,bespoke upper-torso humanoid robot,constrained inverse-kinematic solution,conversational gestures,gesture features,gesture recognition,gesture trajectories,human-robot interaction,humanoid robots,humanoid torso,low computational-cost gesture production algorith,motion control,physically embodied robots,robot dynamics,virtual agents},
mendeley-tags = {BERTI robot},
month = oct,
pages = {1645--1649},
publisher = {IEEE},
shorttitle = {Systems, Man and Cybernetics, 2009. SMC 2009. IEEE},
title = {{Conversational gestures in human-robot interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5346903},
year = {2009}
}
@article{Kim2012,
author = {Kim, Heon‐Hui and Ha, Yun‐Su and Bien, Zeungnam and Park, Kwang‐Hyun},
doi = {10.1108/01439911211268705},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2012 - Gesture encoding and reproduction for human‐robot interaction in text‐to‐gesture systems.pdf:pdf},
issn = {0143-991X},
journal = {Industrial Robot: An International Journal},
keywords = {TTG,gesture encoding,gesture reproduction,text-to-gesture},
mendeley-tags = {TTG,gesture encoding,gesture reproduction,text-to-gesture},
month = oct,
number = {6},
pages = {551--563},
title = {{Gesture encoding and reproduction for human‐robot interaction in text‐to‐gesture systems}},
url = {http://www.researchgate.net/publication/263180797\_Gesture\_encoding\_and\_reproduction\_for\_human-robot\_interaction\_in\_text-to-gesture\_systems},
volume = {39},
year = {2012}
}
@inproceedings{Kwak2008,
abstract = {The objective of this study is to express the four types of personality of a robot based on Myers-Briggs Type Indicator by controlling the size, speed, and frequency of the gestures of a robot and to examine userpsila impressions of the robot by controlling the gesture design factors. The independent variables were three gesture design factors (speed, velocity, and frequency) with two levels each, producing eight gesture types. The eight gesture types were presented in two robot positions, a speaking robot and a listening robot. Participants watched a total of 16 robot gesture samples and evaluated each sample on the personality of the robot and their impressions. The evaluation results provided basic guidelines to express robot personalities through gestures, describing specific methods to control the speed, velocity, and frequency of a gesture. In addition, it was found that personality expression of a robot through gesture and the control of gesture design factors affect user impression of a robot.},
author = {Kwak, Sonya S.},
booktitle = {RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2008.4600715},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Kwak - 2008 - Personality design of sociable robots by control of gesture design factors.pdf:pdf},
isbn = {978-1-4244-2212-8},
keywords = {Communication industry,Communication system control,Frequency,Human robot interaction,Industrial control,Motion control,Myers-Briggs type indicator,Robot control,Service robots,Size control,Speech,gesture design factors,listening robot,man-machine systems,personality design,psychology,robot gesture,robot personalities,robots,sociable robots,speaking robot},
month = aug,
pages = {494--499},
publisher = {IEEE},
shorttitle = {Robot and Human Interactive Communication, 2008. R},
title = {{Personality design of sociable robots by control of gesture design factors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4600715},
year = {2008}
}
@inproceedings{Le2011,
abstract = {We aim at equipping the humanoid robot NAO with the capacity of performing expressive communicative gestures while telling a story. Given a set of intentions and emotions to convey, our system selects the corresponding gestures from a gestural database, called lexicon. Then it calculates the gestures to be expressive and plans their timing to be synchronized with speech. After that the gestures are instantiated as robot joint values and sent to the robot in order to execute the hand-arm movements. The robot has certain physical constraints to be addressed such as the limits of movement space and joint speed. This article presents our ongoing work on a gesture model generating co-verbal gestures for the robot while taking into account these constraints.},
author = {Le, Quoc Anh and Hanoune, Souheil and Pelachaud, Catherine},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100857},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Le, Hanoune, Pelachaud - 2011 - Design and implementation of an expressive gesture model for a humanoid robot.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {2164-0572},
keywords = {Animation,BML,GRETA,Joints,NAO,Robots,SAIBA,Speech,Synchronization,Wrist,coverbal gestures,expressive gesture model,expressivity,gestural database,gesture,gesture recognition,hand-arm movements,humanoid,humanoid robot NAO,humanoid robots,intelligent robots,joint speed,lexicon,movement space,nao,physical constraints,robot joint values},
mendeley-tags = {nao},
month = oct,
pages = {134--140},
publisher = {IEEE},
shorttitle = {Humanoid Robots (Humanoids), 2011 11th IEEE-RAS In},
title = {{Design and implementation of an expressive gesture model for a humanoid robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100857},
year = {2011}
}
@inproceedings{Meena2012,
abstract = {We present an approach to enhance the interaction abilities of the Nao humanoid robot by extending its communicative behavior with non-verbal gestures (hand and head movements, and gaze following). A set of non-verbal gestures were identified that Nao could use for enhancing its presentation and turn-management capabilities in conversational interactions. We discuss our approach for modeling and synthesizing gestures on the Nao robot. A scheme for system evaluation that compares the values of users' expectations and actual experiences has been presented. We found that open arm gestures, head movements and gaze following could significantly enhance Nao's ability to be expressive and appear lively, and to engage human users in conversational interactions.},
author = {Meena, Raveesh and Jokinen, Kristiina and Wilcock, Graham},
booktitle = {2012 IEEE 3rd International Conference on Cognitive Infocommunications (CogInfoCom)},
doi = {10.1109/CogInfoCom.2012.6421936},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Meena, Jokinen, Wilcock - 2012 - Integration of gestures and speech in human-robot interaction.pdf:pdf},
isbn = {978-1-4673-5188-1},
keywords = {nao},
mendeley-tags = {nao},
month = dec,
pages = {673--678},
publisher = {IEEE},
shorttitle = {Cognitive Infocommunications (CogInfoCom), 2012 IE},
title = {{Integration of gestures and speech in human-robot interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6421936},
year = {2012}
}
@inproceedings{Ng-Thow-Hing2010,
abstract = {We present a model that is capable of synchronizing expressive gestures with speech. The model, implemented on a Honda humanoid robot, can generate a full range of gesture types, such as emblems, iconic and metaphoric gestures, deictic pointing and beat gestures. Arbitrary input text is analyzed with a part-of-speech tagger and a text-to-speech engine for timing information of spoken words. In addition, style tags can be optionally added to specify the level of excitement or topic changes. The text, combined with any tags, is then processed by several grammars, one for each gesture type to produce several candidate gestures for each word of the text. The model then selects probabilistically amongst the gesture types based on the desired degree of expressivity. Once a gesture type is selected, it coincides with a particular gesture template, consisting of trajectory curves that define the gesture. Speech timing patterns and style parameters are used to modulate the shape of the curve before it sent to the whole body control system on the robot. Evaluation of the model's parameters were performed, demonstrating the ability of observers to differentiate varying levels of expressiveness, excitement and speech synchronization. Modification of gesture speed for trajectory tracking found that positive associations like happiness and excitement accompanied faster speeds, with negative associations like sadness or tiredness occurred at slower speeds.},
author = {Ng-Thow-Hing, Victor and Okita, Sandra},
booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2010.5654322},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Ng-Thow-Hing, Okita - 2010 - Synchronized gesture and speech production for humanoid robots.pdf:pdf},
isbn = {978-1-4244-6674-0},
issn = {2153-0858},
keywords = {Honda robot,gesture recognition,gesture synchronization,gesture template,humanoid robots,object tracking,part-of-speech tagger,position control,speech processing,speech production,speech synchronization,synchronisation,text-to-speech engine,trajectory tracking},
mendeley-tags = {Honda robot},
month = oct,
pages = {4617--4624},
publisher = {IEEE},
shorttitle = {Intelligent Robots and Systems (IROS), 2010 IEEE/R},
title = {{Synchronized gesture and speech production for humanoid robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5654322},
year = {2010}
}
@inproceedings{Tay2012,
abstract = {We formalize the representation of gestures and present a model that is capable of synchronizing expressive and relevant gestures with text-to-speech input. A gesture consists of gesture primitives that are executed simultaneously. We formally define the gesture primitive and introduce the concept of a spatially targeted gesture primitive, i.e., a gesture primitive that is directed at a target of interest. The spatially targeted gesture primitive is useful for situations where the direction of the gesture is important for meaningful human-robot interaction. We contribute an algorithm to determine how a spatially targeted gesture primitive is generated. We also contribute a process to analyze the input text, determine relevant gesture primitives from the input text, compose gestures from gesture primitives and rank the combinations of gestures. We propose a set of criteria that weights and ranks the combinations of gestures. Although we illustrate the utility of our model, algorithm and process using a NAO humanoid robot, our contributions are applicable to other robots.},
author = {Tay, Junyun and Veloso, Manuela},
booktitle = {2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2012.6343739},
file = {:Users/lpmayos/Library/Application Support/Mendeley Desktop/Downloaded/Tay, Veloso - 2012 - Modeling and composing gestures for human-robot interaction.pdf:pdf},
isbn = {978-1-4673-4606-1},
issn = {1944-9445},
keywords = {Collision avoidance,Joints,NAO humanoid robot,Robot kinematics,Speech,Timing,Vectors,gesture composition,gesture modeling,human-robot interaction,humanoid robots,nao,spatially targeted gesture primitive,speech synthesis,text-to-speech input},
mendeley-tags = {nao},
month = sep,
pages = {107--112},
publisher = {IEEE},
shorttitle = {RO-MAN, 2012 IEEE},
title = {{Modeling and composing gestures for human-robot interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6343739},
year = {2012}
}
